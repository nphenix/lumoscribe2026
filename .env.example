# ==============================================================================
# Lumoscribe AI 文档生成平台配置
# ==============================================================================

# ============== 应用配置 ==============
LUMO_API_HOST=127.0.0.1
LUMO_API_PORT=7901
LUMO_LOG_LEVEL=INFO
LUMO_API_MODE=full
LUMO_API_RELOAD=0

# ============== 白皮书生成（whitepaper）服务端默认参数 ==============
# 用于降低前端集成成本：前端不传 top_k/rerank_top_n/polish_outline 时，服务端使用这些默认值。
LUMO_WHITEPAPER_TOP_K=50
LUMO_WHITEPAPER_RERANK_TOP_N=50
LUMO_WHITEPAPER_POLISH_OUTLINE=0

# ============== 存储配置 ==============
LUMO_STORAGE_ROOT=.runtime/storage

# ============== 数据库配置 ==============
LUMO_SQLITE_PATH=.runtime/sqlite/lumoscribe.db

# ============== 消息队列配置 ==============
LUMO_REDIS_URL=redis://127.0.0.1:6379/0

# ==============================================================================
# LLM 配置
# ==============================================================================
# 重要：
# - Provider/Model/Capability/Prompt 本身存储在 SQLite，需要通过 API 创建（/v1/llm/*、/v1/prompts）
# - `.env` 只负责提供“密钥/默认地址”等运行时环境变量；实际使用哪个变量名由 provider.api_key_env 决定
# Provider Type: openai_compatible / ollama / flagembedding / llamacpp / huggingface / mineru
# Model Kind: chat / embedding / rerank / multimodal
# Capability: inference / embedding / rerank / ocr / multimodal / doc_cleaning / chart_json
# ==============================================================================

# ============== OpenAI Compatible (Chat + Multimodal) ==============
# Provider 额外配置（在中台 Provider 的 config_json / config 字段里填写）示例：
#   {"temperature": 0.2, "max_tokens": 2048, "timeout_seconds": 60, "stream": false}
#
# API Key 环境变量名（创建 Provider 时设置 api_key_env 为此值）
LUMO_LLM_OPENAI_API_KEY=

# OpenAI API Base URL（可选，使用官方 API 则留空）
# LUMO_LLM_OPENAI_BASE_URL=https://api.openai.com/v1

# ============== Ollama (Chat) ==============
# Provider 额外配置示例：
#   {"ollama_model": "llama3.1"}  # 可作为默认模型（也可直接在 Model 表中建模）
LUMO_LLM_OLLAMA_BASE_URL=http://localhost:11434

# ============== LlamaCpp (本地 GGUF 模型) ==============
# LlamaCpp 模型文件路径（创建 Model 时 name 字段填此路径）
# LUMO_LLM_LLAMACPP_MODEL_PATH=./models/llama-2-7b-chat.Q4_0.gguf
#
# 安装 llama-cpp-python（GPU 支持）：
#   # 使用 cuBLAS (NVIDIA GPU)
#   CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python
#
#   # 使用 Metal (Apple Silicon)
#   CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-python
#
# Model 配置示例（GPU）：
#   name: "./models/llama-2-7b-chat.Q4_0.gguf"
#   provider_type: "llamacpp"
#   model_kind: "chat"
#   config_json: {"n_gpu_layers": -1, "n_batch": 512, "temperature": 0.7}
#   # n_gpu_layers: -1 表示将所有层加载到 GPU（加速推理）
#   # n_batch: 批处理大小，根据显存调整

# ============== FlagEmbedding (Embedding + Rerank) ==============
# FlagEmbedding API Server（远程部署）
LUMO_LLM_FLAGEMBEDDING_HOST=http://localhost:7904
#
# FlagEmbedding 本地 GPU 配置：
# Model 配置示例（GPU）：
#   name: "BAAI/bge-large-zh"
#   provider_type: "flagembedding"
#   model_kind: "embedding"
#   config_json: {"use_fp16": true, "device": "cuda", "trust_remote_code": true}
#   # device: "cuda" 或 "cuda:0" = NVIDIA GPU
#   # device: "mps" = Apple Silicon GPU
#   # device: "cpu" = CPU（默认）
#   # trust_remote_code: 是否信任远程代码（某些模型需要）
#
# 安装 GPU 依赖：
#   # PyTorch GPU 版本
#   pip install torch --index-url https://download.pytorch.org/whl/cu118
#
#   # 或安装 FlagEmbedding 时指定 GPU
#   pip install FlagEmbedding --extra-index-url https://download.pytorch.org/whl/cu118

# ============== HuggingFace (Embeddings) ==============
# HuggingFace 模型缓存目录
# LUMO_LLM_HUGGINGFACE_CACHE=./models/huggingface

# ==============================================================================
# MinerU 配置（PDF OCR）
# ==============================================================================
# MinerU 在线服务 API 地址
LUMO_MINERU_API_URL=http://localhost:7905
# MinerU API Key（如需认证）
# LUMO_MINERU_API_KEY=
# MinerU 处理超时时间（秒）
LUMO_MINERU_TIMEOUT=300

# ==============================================================================
# 业务能力模型映射示例
# ==============================================================================
# 请通过 API 创建 Provider/Model/Capability，以下为配置示例：
#
# --- Provider 配置 ---
#
# Provider (OpenAI Chat):
#   name: "openai-gpt4"
#   provider_type: "openai_compatible"
#   base_url: "https://api.openai.com/v1"
#   api_key_env: "LUMO_LLM_OPENAI_API_KEY"
#
# Provider (LlamaCpp 本地模型):
#   name: "llama-cpp-local"
#   provider_type: "llamacpp"
#   base_url: ""
#
# Provider (FlagEmbedding):
#   name: "flag-embedding-server"
#   provider_type: "flagembedding"
#   base_url: "http://localhost:7904"
#
# --- Model 配置 ---
#
# Model (Chat - 推理):
#   name: "gpt-4o"
#   provider_id: "<openai-gpt4-id>"
#   model_kind: "chat"
#
# Model (Chat - 轻量本地):
#   name: "./models/llama-2-7b-chat.Q4_0.gguf"
#   provider_id: "<llama-cpp-local-id>"
#   model_kind: "chat"
#   config_json: {"temperature": 0.7, "n_ctx": 4096}
#
# Model (Embedding - FlagEmbedding):
#   name: "BAAI/bge-large-zh"
#   provider_id: "<flag-embedding-server-id>"
#   model_kind: "embedding"
#   config_json: {"use_fp16": true}
#
# Model (Rerank - FlagEmbedding):
#   name: "BAAI/bge-reranker-base"
#   provider_id: "<flag-embedding-server-id>"
#   model_kind: "rerank"
#   config_json: {"max_length": 512}
#
# Model (Multimodal - GPT-4V):
#   name: "gpt-4o"
#   provider_id: "<openai-gpt4-id>"
#   model_kind: "multimodal"
#
# --- Capability 配置 ---
#
# Capability (PDF OCR):
#   capability: "ocr"
#   model_id: "<mineru-model-id>"  # MinerU 为外部服务，无需模型
#   priority: 10
#
# Capability (文档清洗):
#   capability: "doc_cleaning"
#   model_id: "<gpt-4o-id>"
#   priority: 10
#
# Capability (模板润色):
#   capability: "polish"
#   model_id: "<gpt-4o-id>"
#   priority: 20
#
# Capability (图表转 JSON):
#   capability: "chart_json"
#   model_id: "<gpt-4o-id>"
#   priority: 10
#
# Capability (长文档生成):
#   capability: "doc_generation"
#   model_id: "<gpt-4o-id>"
#   priority: 10
#
# Capability (知识库向量):
#   capability: "embedding"
#   model_id: "<bge-large-id>"
#   priority: 10
#
# Capability (检索重排序):
#   capability: "rerank"
#   model_id: "<bge-reranker-id>"
#   priority: 10
